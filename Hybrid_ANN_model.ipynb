{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d42844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# We will use the sklearn preprocessing library, as it will be easier to standardize the data.\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "raw_excel_data = pd.read_excel('Database.xlsx')\n",
    "\n",
    "# Replace missing values in the columns with their respective mean\n",
    "#raw_excel_data.fillna(raw_excel_data.mean(), inplace=True)\n",
    "\n",
    "raw_excel_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef6bb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values = raw_excel_data.isnull().sum()\n",
    "\n",
    "# Display columns with missing values and their counts\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "408c858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs_all = raw_excel_data.iloc[:,0:-1].values\n",
    "\n",
    "# The targets are in the last column. That's how datasets are conventionally organized.\n",
    "unscaled_targets_all = raw_excel_data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8bdb04c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01190476 -1.         -1.         -1.          0.39534884  0.42779292\n",
      "  -1.         -1.         -0.11164466 -1.         -1.        ]\n",
      " [-0.01190476 -1.         -1.         -1.          0.39534884  0.14441417\n",
      "  -0.68125    -0.32347826 -0.11164466 -1.         -1.        ]\n",
      " [-0.01190476 -1.         -1.         -1.          0.39534884 -0.28610354\n",
      "  -0.209375   -0.32347826 -0.11164466 -1.         -1.        ]\n",
      " [-0.01190476 -1.         -1.         -1.          0.39534884 -1.\n",
      "   0.5890625  -0.32347826 -0.11164466 -1.         -1.        ]\n",
      " [-0.31696429 -1.         -1.         -0.45333333  0.39534884  0.42779292\n",
      "  -1.         -1.         -0.18607443 -1.         -1.        ]]\n",
      "[[-0.87267736]\n",
      " [-0.88700115]\n",
      " [-0.89177575]\n",
      " [-0.91405722]\n",
      " [-0.88381809]]\n"
     ]
    }
   ],
   "source": [
    "# Define the StandardScaler object for inputs and targets\n",
    "#scaler_inputs = preprocessing.StandardScaler()\n",
    "#scaler_targets = preprocessing.StandardScaler()\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Continue with scaling and calculations\n",
    "scaled_inputs = scaler.fit_transform(unscaled_inputs_all)\n",
    "scaled_targets = scaler.fit_transform(unscaled_targets_all.reshape(-1, 1))\n",
    "print(scaled_inputs[:5])\n",
    "print(scaled_targets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f523ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the data was collected it was actually arranged by date\n",
    "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n",
    "# Since we will be batching, we want the data to be as randomly spread out as possible\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = scaled_targets [shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb3e1908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Count the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
    "# Naturally, the numbers are integers.\n",
    "train_samples_count = int(0.7 * samples_count)\n",
    "validation_samples_count = int(0.15 * samples_count)\n",
    "\n",
    "# The 'test' dataset contains all remaining data.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# Create variables that record the inputs and targets for training\n",
    "# In our shuffled dataset, they are the first \"train_samples_count\" observations\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "# They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "# They are everything that is remaining.\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were \n",
    "# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code, \n",
    "# you will get different values, as each time they are shuffled randomly.\n",
    "# Normally you preprocess ONCE, so you need not rerun this code once it is done.\n",
    "# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5498fa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "17/17 - 1s - loss: 0.2140 - mse: 0.2140 - val_loss: 0.1227 - val_mse: 0.1227 - 599ms/epoch - 35ms/step\n",
      "Epoch 2/1000\n",
      "17/17 - 0s - loss: 0.1019 - mse: 0.1019 - val_loss: 0.0955 - val_mse: 0.0955 - 60ms/epoch - 4ms/step\n",
      "Epoch 3/1000\n",
      "17/17 - 0s - loss: 0.0863 - mse: 0.0863 - val_loss: 0.0796 - val_mse: 0.0796 - 51ms/epoch - 3ms/step\n",
      "Epoch 4/1000\n",
      "17/17 - 0s - loss: 0.0787 - mse: 0.0787 - val_loss: 0.0767 - val_mse: 0.0767 - 50ms/epoch - 3ms/step\n",
      "Epoch 5/1000\n",
      "17/17 - 0s - loss: 0.0777 - mse: 0.0777 - val_loss: 0.0753 - val_mse: 0.0753 - 51ms/epoch - 3ms/step\n",
      "Epoch 6/1000\n",
      "17/17 - 0s - loss: 0.0782 - mse: 0.0782 - val_loss: 0.0736 - val_mse: 0.0736 - 51ms/epoch - 3ms/step\n",
      "Epoch 7/1000\n",
      "17/17 - 0s - loss: 0.0743 - mse: 0.0743 - val_loss: 0.0717 - val_mse: 0.0717 - 54ms/epoch - 3ms/step\n",
      "Epoch 8/1000\n",
      "17/17 - 0s - loss: 0.0725 - mse: 0.0725 - val_loss: 0.0700 - val_mse: 0.0700 - 53ms/epoch - 3ms/step\n",
      "Epoch 9/1000\n",
      "17/17 - 0s - loss: 0.0719 - mse: 0.0719 - val_loss: 0.0689 - val_mse: 0.0689 - 51ms/epoch - 3ms/step\n",
      "Epoch 10/1000\n",
      "17/17 - 0s - loss: 0.0706 - mse: 0.0706 - val_loss: 0.0676 - val_mse: 0.0676 - 52ms/epoch - 3ms/step\n",
      "Epoch 11/1000\n",
      "17/17 - 0s - loss: 0.0680 - mse: 0.0680 - val_loss: 0.0681 - val_mse: 0.0681 - 53ms/epoch - 3ms/step\n",
      "Epoch 12/1000\n",
      "17/17 - 0s - loss: 0.0672 - mse: 0.0672 - val_loss: 0.0652 - val_mse: 0.0652 - 55ms/epoch - 3ms/step\n",
      "Epoch 13/1000\n",
      "17/17 - 0s - loss: 0.0663 - mse: 0.0663 - val_loss: 0.0664 - val_mse: 0.0664 - 51ms/epoch - 3ms/step\n",
      "Epoch 14/1000\n",
      "17/17 - 0s - loss: 0.0651 - mse: 0.0651 - val_loss: 0.0639 - val_mse: 0.0639 - 53ms/epoch - 3ms/step\n",
      "Epoch 15/1000\n",
      "17/17 - 0s - loss: 0.0649 - mse: 0.0649 - val_loss: 0.0643 - val_mse: 0.0643 - 56ms/epoch - 3ms/step\n",
      "Epoch 16/1000\n",
      "17/17 - 0s - loss: 0.0627 - mse: 0.0627 - val_loss: 0.0625 - val_mse: 0.0625 - 55ms/epoch - 3ms/step\n",
      "Epoch 17/1000\n",
      "17/17 - 0s - loss: 0.0632 - mse: 0.0632 - val_loss: 0.0640 - val_mse: 0.0640 - 50ms/epoch - 3ms/step\n",
      "Epoch 18/1000\n",
      "17/17 - 0s - loss: 0.0629 - mse: 0.0629 - val_loss: 0.0615 - val_mse: 0.0615 - 48ms/epoch - 3ms/step\n",
      "Epoch 19/1000\n",
      "17/17 - 0s - loss: 0.0617 - mse: 0.0617 - val_loss: 0.0636 - val_mse: 0.0636 - 52ms/epoch - 3ms/step\n",
      "Epoch 20/1000\n",
      "17/17 - 0s - loss: 0.0608 - mse: 0.0608 - val_loss: 0.0615 - val_mse: 0.0615 - 53ms/epoch - 3ms/step\n",
      "Epoch 21/1000\n",
      "17/17 - 0s - loss: 0.0608 - mse: 0.0608 - val_loss: 0.0641 - val_mse: 0.0641 - 55ms/epoch - 3ms/step\n",
      "Epoch 22/1000\n",
      "17/17 - 0s - loss: 0.0629 - mse: 0.0629 - val_loss: 0.0606 - val_mse: 0.0606 - 58ms/epoch - 3ms/step\n",
      "Epoch 23/1000\n",
      "17/17 - 0s - loss: 0.0604 - mse: 0.0604 - val_loss: 0.0619 - val_mse: 0.0619 - 61ms/epoch - 4ms/step\n",
      "Epoch 24/1000\n",
      "17/17 - 0s - loss: 0.0602 - mse: 0.0602 - val_loss: 0.0602 - val_mse: 0.0602 - 59ms/epoch - 3ms/step\n",
      "Epoch 25/1000\n",
      "17/17 - 0s - loss: 0.0597 - mse: 0.0597 - val_loss: 0.0604 - val_mse: 0.0604 - 64ms/epoch - 4ms/step\n",
      "Epoch 26/1000\n",
      "17/17 - 0s - loss: 0.0602 - mse: 0.0602 - val_loss: 0.0600 - val_mse: 0.0600 - 58ms/epoch - 3ms/step\n",
      "Epoch 27/1000\n",
      "17/17 - 0s - loss: 0.0604 - mse: 0.0604 - val_loss: 0.0680 - val_mse: 0.0680 - 53ms/epoch - 3ms/step\n",
      "Epoch 28/1000\n",
      "17/17 - 0s - loss: 0.0599 - mse: 0.0599 - val_loss: 0.0607 - val_mse: 0.0607 - 54ms/epoch - 3ms/step\n",
      "Epoch 29/1000\n",
      "17/17 - 0s - loss: 0.0591 - mse: 0.0591 - val_loss: 0.0601 - val_mse: 0.0601 - 49ms/epoch - 3ms/step\n",
      "Epoch 30/1000\n",
      "17/17 - 0s - loss: 0.0586 - mse: 0.0586 - val_loss: 0.0603 - val_mse: 0.0603 - 52ms/epoch - 3ms/step\n",
      "Epoch 31/1000\n",
      "17/17 - 0s - loss: 0.0583 - mse: 0.0583 - val_loss: 0.0602 - val_mse: 0.0602 - 50ms/epoch - 3ms/step\n",
      "Epoch 32/1000\n",
      "17/17 - 0s - loss: 0.0587 - mse: 0.0587 - val_loss: 0.0590 - val_mse: 0.0590 - 52ms/epoch - 3ms/step\n",
      "Epoch 33/1000\n",
      "17/17 - 0s - loss: 0.0586 - mse: 0.0586 - val_loss: 0.0589 - val_mse: 0.0589 - 51ms/epoch - 3ms/step\n",
      "Epoch 34/1000\n",
      "17/17 - 0s - loss: 0.0593 - mse: 0.0593 - val_loss: 0.0600 - val_mse: 0.0600 - 109ms/epoch - 6ms/step\n",
      "Epoch 35/1000\n",
      "17/17 - 0s - loss: 0.0583 - mse: 0.0583 - val_loss: 0.0586 - val_mse: 0.0586 - 73ms/epoch - 4ms/step\n",
      "Epoch 36/1000\n",
      "17/17 - 0s - loss: 0.0583 - mse: 0.0583 - val_loss: 0.0622 - val_mse: 0.0622 - 53ms/epoch - 3ms/step\n",
      "Epoch 37/1000\n",
      "17/17 - 0s - loss: 0.0579 - mse: 0.0579 - val_loss: 0.0582 - val_mse: 0.0582 - 51ms/epoch - 3ms/step\n",
      "Epoch 38/1000\n",
      "17/17 - 0s - loss: 0.0577 - mse: 0.0577 - val_loss: 0.0583 - val_mse: 0.0583 - 67ms/epoch - 4ms/step\n",
      "Epoch 39/1000\n",
      "17/17 - 0s - loss: 0.0584 - mse: 0.0584 - val_loss: 0.0588 - val_mse: 0.0588 - 52ms/epoch - 3ms/step\n",
      "Epoch 40/1000\n",
      "17/17 - 0s - loss: 0.0603 - mse: 0.0603 - val_loss: 0.0577 - val_mse: 0.0577 - 52ms/epoch - 3ms/step\n",
      "Epoch 41/1000\n",
      "17/17 - 0s - loss: 0.0577 - mse: 0.0577 - val_loss: 0.0583 - val_mse: 0.0583 - 55ms/epoch - 3ms/step\n",
      "Epoch 42/1000\n",
      "17/17 - 0s - loss: 0.0571 - mse: 0.0571 - val_loss: 0.0577 - val_mse: 0.0577 - 56ms/epoch - 3ms/step\n",
      "Epoch 43/1000\n",
      "17/17 - 0s - loss: 0.0570 - mse: 0.0570 - val_loss: 0.0591 - val_mse: 0.0591 - 54ms/epoch - 3ms/step\n",
      "Epoch 44/1000\n",
      "17/17 - 0s - loss: 0.0570 - mse: 0.0570 - val_loss: 0.0573 - val_mse: 0.0573 - 54ms/epoch - 3ms/step\n",
      "Epoch 45/1000\n",
      "17/17 - 0s - loss: 0.0563 - mse: 0.0563 - val_loss: 0.0576 - val_mse: 0.0576 - 51ms/epoch - 3ms/step\n",
      "Epoch 46/1000\n",
      "17/17 - 0s - loss: 0.0567 - mse: 0.0567 - val_loss: 0.0579 - val_mse: 0.0579 - 51ms/epoch - 3ms/step\n",
      "Epoch 47/1000\n",
      "17/17 - 0s - loss: 0.0563 - mse: 0.0563 - val_loss: 0.0572 - val_mse: 0.0572 - 50ms/epoch - 3ms/step\n",
      "Epoch 48/1000\n",
      "17/17 - 0s - loss: 0.0569 - mse: 0.0569 - val_loss: 0.0630 - val_mse: 0.0630 - 51ms/epoch - 3ms/step\n",
      "Epoch 49/1000\n",
      "17/17 - 0s - loss: 0.0594 - mse: 0.0594 - val_loss: 0.0614 - val_mse: 0.0614 - 51ms/epoch - 3ms/step\n",
      "Epoch 50/1000\n",
      "17/17 - 0s - loss: 0.0614 - mse: 0.0614 - val_loss: 0.0628 - val_mse: 0.0628 - 51ms/epoch - 3ms/step\n",
      "Epoch 51/1000\n",
      "17/17 - 0s - loss: 0.0606 - mse: 0.0606 - val_loss: 0.0655 - val_mse: 0.0655 - 55ms/epoch - 3ms/step\n",
      "Epoch 52/1000\n",
      "17/17 - 0s - loss: 0.0582 - mse: 0.0582 - val_loss: 0.0582 - val_mse: 0.0582 - 54ms/epoch - 3ms/step\n",
      "Epoch 53/1000\n",
      "17/17 - 0s - loss: 0.0569 - mse: 0.0569 - val_loss: 0.0566 - val_mse: 0.0566 - 52ms/epoch - 3ms/step\n",
      "Epoch 54/1000\n",
      "17/17 - 0s - loss: 0.0580 - mse: 0.0580 - val_loss: 0.0571 - val_mse: 0.0571 - 54ms/epoch - 3ms/step\n",
      "Epoch 55/1000\n",
      "17/17 - 0s - loss: 0.0597 - mse: 0.0597 - val_loss: 0.0647 - val_mse: 0.0647 - 49ms/epoch - 3ms/step\n",
      "Epoch 56/1000\n",
      "17/17 - 0s - loss: 0.0604 - mse: 0.0604 - val_loss: 0.0586 - val_mse: 0.0586 - 53ms/epoch - 3ms/step\n",
      "Epoch 57/1000\n",
      "17/17 - 0s - loss: 0.0570 - mse: 0.0570 - val_loss: 0.0564 - val_mse: 0.0564 - 56ms/epoch - 3ms/step\n",
      "Epoch 58/1000\n",
      "17/17 - 0s - loss: 0.0574 - mse: 0.0574 - val_loss: 0.0592 - val_mse: 0.0592 - 53ms/epoch - 3ms/step\n",
      "Epoch 59/1000\n",
      "17/17 - 0s - loss: 0.0566 - mse: 0.0566 - val_loss: 0.0576 - val_mse: 0.0576 - 46ms/epoch - 3ms/step\n",
      "Epoch 60/1000\n",
      "17/17 - 0s - loss: 0.0562 - mse: 0.0562 - val_loss: 0.0608 - val_mse: 0.0608 - 48ms/epoch - 3ms/step\n",
      "Epoch 61/1000\n",
      "17/17 - 0s - loss: 0.0568 - mse: 0.0568 - val_loss: 0.0584 - val_mse: 0.0584 - 54ms/epoch - 3ms/step\n",
      "Epoch 62/1000\n",
      "17/17 - 0s - loss: 0.0574 - mse: 0.0574 - val_loss: 0.0657 - val_mse: 0.0657 - 51ms/epoch - 3ms/step\n",
      "Epoch 63/1000\n",
      "17/17 - 0s - loss: 0.0613 - mse: 0.0613 - val_loss: 0.0701 - val_mse: 0.0701 - 51ms/epoch - 3ms/step\n",
      "Epoch 64/1000\n",
      "17/17 - 0s - loss: 0.0574 - mse: 0.0574 - val_loss: 0.0606 - val_mse: 0.0606 - 54ms/epoch - 3ms/step\n",
      "Epoch 65/1000\n",
      "17/17 - 0s - loss: 0.0570 - mse: 0.0570 - val_loss: 0.0571 - val_mse: 0.0571 - 54ms/epoch - 3ms/step\n",
      "Epoch 66/1000\n",
      "17/17 - 0s - loss: 0.0559 - mse: 0.0559 - val_loss: 0.0654 - val_mse: 0.0654 - 55ms/epoch - 3ms/step\n",
      "Epoch 67/1000\n",
      "17/17 - 0s - loss: 0.0602 - mse: 0.0602 - val_loss: 0.0563 - val_mse: 0.0563 - 49ms/epoch - 3ms/step\n",
      "Epoch 68/1000\n",
      "17/17 - 0s - loss: 0.0595 - mse: 0.0595 - val_loss: 0.0563 - val_mse: 0.0563 - 50ms/epoch - 3ms/step\n",
      "Epoch 69/1000\n",
      "17/17 - 0s - loss: 0.0571 - mse: 0.0571 - val_loss: 0.0574 - val_mse: 0.0574 - 47ms/epoch - 3ms/step\n",
      "Epoch 70/1000\n",
      "17/17 - 0s - loss: 0.0566 - mse: 0.0566 - val_loss: 0.0564 - val_mse: 0.0564 - 48ms/epoch - 3ms/step\n",
      "Epoch 71/1000\n",
      "17/17 - 0s - loss: 0.0611 - mse: 0.0611 - val_loss: 0.0716 - val_mse: 0.0716 - 48ms/epoch - 3ms/step\n",
      "Epoch 72/1000\n",
      "17/17 - 0s - loss: 0.0588 - mse: 0.0588 - val_loss: 0.0563 - val_mse: 0.0563 - 47ms/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/1000\n",
      "17/17 - 0s - loss: 0.0565 - mse: 0.0565 - val_loss: 0.0565 - val_mse: 0.0565 - 50ms/epoch - 3ms/step\n",
      "Epoch 74/1000\n",
      "17/17 - 0s - loss: 0.0552 - mse: 0.0552 - val_loss: 0.0569 - val_mse: 0.0569 - 53ms/epoch - 3ms/step\n",
      "Epoch 75/1000\n",
      "17/17 - 0s - loss: 0.0569 - mse: 0.0569 - val_loss: 0.0562 - val_mse: 0.0562 - 56ms/epoch - 3ms/step\n",
      "Epoch 76/1000\n",
      "17/17 - 0s - loss: 0.0570 - mse: 0.0570 - val_loss: 0.0624 - val_mse: 0.0624 - 63ms/epoch - 4ms/step\n",
      "Epoch 77/1000\n",
      "17/17 - 0s - loss: 0.0589 - mse: 0.0589 - val_loss: 0.0570 - val_mse: 0.0570 - 58ms/epoch - 3ms/step\n",
      "Epoch 78/1000\n",
      "17/17 - 0s - loss: 0.0561 - mse: 0.0561 - val_loss: 0.0560 - val_mse: 0.0560 - 52ms/epoch - 3ms/step\n",
      "Epoch 79/1000\n",
      "17/17 - 0s - loss: 0.0572 - mse: 0.0572 - val_loss: 0.0650 - val_mse: 0.0650 - 70ms/epoch - 4ms/step\n",
      "Epoch 80/1000\n",
      "17/17 - 0s - loss: 0.0636 - mse: 0.0636 - val_loss: 0.0641 - val_mse: 0.0641 - 47ms/epoch - 3ms/step\n",
      "Epoch 81/1000\n",
      "17/17 - 0s - loss: 0.0577 - mse: 0.0577 - val_loss: 0.0586 - val_mse: 0.0586 - 48ms/epoch - 3ms/step\n",
      "Epoch 82/1000\n",
      "17/17 - 0s - loss: 0.0568 - mse: 0.0568 - val_loss: 0.0630 - val_mse: 0.0630 - 51ms/epoch - 3ms/step\n",
      "Epoch 83/1000\n",
      "17/17 - 0s - loss: 0.0584 - mse: 0.0584 - val_loss: 0.0583 - val_mse: 0.0583 - 46ms/epoch - 3ms/step\n",
      "Epoch 84/1000\n",
      "17/17 - 0s - loss: 0.0597 - mse: 0.0597 - val_loss: 0.0636 - val_mse: 0.0636 - 48ms/epoch - 3ms/step\n",
      "Epoch 85/1000\n",
      "17/17 - 0s - loss: 0.0578 - mse: 0.0578 - val_loss: 0.0616 - val_mse: 0.0616 - 49ms/epoch - 3ms/step\n",
      "Epoch 86/1000\n",
      "17/17 - 0s - loss: 0.0592 - mse: 0.0592 - val_loss: 0.0598 - val_mse: 0.0598 - 50ms/epoch - 3ms/step\n",
      "Epoch 87/1000\n",
      "17/17 - 0s - loss: 0.0560 - mse: 0.0560 - val_loss: 0.0562 - val_mse: 0.0562 - 55ms/epoch - 3ms/step\n",
      "Epoch 88/1000\n",
      "17/17 - 0s - loss: 0.0555 - mse: 0.0555 - val_loss: 0.0558 - val_mse: 0.0558 - 52ms/epoch - 3ms/step\n",
      "Epoch 89/1000\n",
      "17/17 - 0s - loss: 0.0570 - mse: 0.0570 - val_loss: 0.0565 - val_mse: 0.0565 - 47ms/epoch - 3ms/step\n",
      "Epoch 90/1000\n",
      "17/17 - 0s - loss: 0.0580 - mse: 0.0580 - val_loss: 0.0569 - val_mse: 0.0569 - 50ms/epoch - 3ms/step\n",
      "Epoch 91/1000\n",
      "17/17 - 0s - loss: 0.0623 - mse: 0.0623 - val_loss: 0.0560 - val_mse: 0.0560 - 52ms/epoch - 3ms/step\n",
      "Epoch 92/1000\n",
      "17/17 - 0s - loss: 0.0610 - mse: 0.0610 - val_loss: 0.0567 - val_mse: 0.0567 - 63ms/epoch - 4ms/step\n",
      "Epoch 93/1000\n",
      "17/17 - 0s - loss: 0.0558 - mse: 0.0558 - val_loss: 0.0561 - val_mse: 0.0561 - 57ms/epoch - 3ms/step\n",
      "Epoch 94/1000\n",
      "17/17 - 0s - loss: 0.0547 - mse: 0.0547 - val_loss: 0.0557 - val_mse: 0.0557 - 53ms/epoch - 3ms/step\n",
      "Epoch 95/1000\n",
      "17/17 - 0s - loss: 0.0551 - mse: 0.0551 - val_loss: 0.0558 - val_mse: 0.0558 - 49ms/epoch - 3ms/step\n",
      "Epoch 96/1000\n",
      "17/17 - 0s - loss: 0.0551 - mse: 0.0551 - val_loss: 0.0560 - val_mse: 0.0560 - 51ms/epoch - 3ms/step\n",
      "Epoch 97/1000\n",
      "17/17 - 0s - loss: 0.0561 - mse: 0.0561 - val_loss: 0.0563 - val_mse: 0.0563 - 50ms/epoch - 3ms/step\n",
      "Epoch 98/1000\n",
      "17/17 - 0s - loss: 0.0554 - mse: 0.0554 - val_loss: 0.0588 - val_mse: 0.0588 - 52ms/epoch - 3ms/step\n",
      "Epoch 99/1000\n",
      "17/17 - 0s - loss: 0.0576 - mse: 0.0576 - val_loss: 0.0594 - val_mse: 0.0594 - 52ms/epoch - 3ms/step\n",
      "Epoch 100/1000\n",
      "17/17 - 0s - loss: 0.0575 - mse: 0.0575 - val_loss: 0.0603 - val_mse: 0.0603 - 50ms/epoch - 3ms/step\n",
      "Epoch 101/1000\n",
      "17/17 - 0s - loss: 0.0559 - mse: 0.0559 - val_loss: 0.0557 - val_mse: 0.0557 - 49ms/epoch - 3ms/step\n",
      "Epoch 102/1000\n",
      "17/17 - 0s - loss: 0.0563 - mse: 0.0563 - val_loss: 0.0558 - val_mse: 0.0558 - 49ms/epoch - 3ms/step\n",
      "Epoch 103/1000\n",
      "17/17 - 0s - loss: 0.0559 - mse: 0.0559 - val_loss: 0.0557 - val_mse: 0.0557 - 48ms/epoch - 3ms/step\n",
      "Epoch 104/1000\n",
      "17/17 - 0s - loss: 0.0556 - mse: 0.0556 - val_loss: 0.0564 - val_mse: 0.0564 - 52ms/epoch - 3ms/step\n",
      "Epoch 105/1000\n",
      "17/17 - 0s - loss: 0.0553 - mse: 0.0553 - val_loss: 0.0561 - val_mse: 0.0561 - 52ms/epoch - 3ms/step\n",
      "Epoch 106/1000\n",
      "17/17 - 0s - loss: 0.0563 - mse: 0.0563 - val_loss: 0.0558 - val_mse: 0.0558 - 47ms/epoch - 3ms/step\n",
      "Epoch 107/1000\n",
      "17/17 - 0s - loss: 0.0555 - mse: 0.0555 - val_loss: 0.0561 - val_mse: 0.0561 - 51ms/epoch - 3ms/step\n",
      "Epoch 108/1000\n",
      "17/17 - 0s - loss: 0.0580 - mse: 0.0580 - val_loss: 0.0563 - val_mse: 0.0563 - 58ms/epoch - 3ms/step\n",
      "Epoch 109/1000\n",
      "17/17 - 0s - loss: 0.0586 - mse: 0.0586 - val_loss: 0.0558 - val_mse: 0.0558 - 60ms/epoch - 4ms/step\n",
      "Epoch 110/1000\n",
      "17/17 - 0s - loss: 0.0564 - mse: 0.0564 - val_loss: 0.0558 - val_mse: 0.0558 - 49ms/epoch - 3ms/step\n",
      "Epoch 111/1000\n",
      "17/17 - 0s - loss: 0.0550 - mse: 0.0550 - val_loss: 0.0562 - val_mse: 0.0562 - 49ms/epoch - 3ms/step\n",
      "Epoch 112/1000\n",
      "17/17 - 0s - loss: 0.0575 - mse: 0.0575 - val_loss: 0.0567 - val_mse: 0.0567 - 48ms/epoch - 3ms/step\n",
      "Epoch 113/1000\n",
      "17/17 - 0s - loss: 0.0579 - mse: 0.0579 - val_loss: 0.0560 - val_mse: 0.0560 - 50ms/epoch - 3ms/step\n",
      "Epoch 114/1000\n",
      "17/17 - 0s - loss: 0.0582 - mse: 0.0582 - val_loss: 0.0565 - val_mse: 0.0565 - 51ms/epoch - 3ms/step\n",
      "Epoch 115/1000\n",
      "17/17 - 0s - loss: 0.0560 - mse: 0.0560 - val_loss: 0.0570 - val_mse: 0.0570 - 48ms/epoch - 3ms/step\n",
      "Epoch 116/1000\n",
      "17/17 - 0s - loss: 0.0565 - mse: 0.0565 - val_loss: 0.0573 - val_mse: 0.0573 - 53ms/epoch - 3ms/step\n",
      "Epoch 117/1000\n",
      "17/17 - 0s - loss: 0.0554 - mse: 0.0554 - val_loss: 0.0557 - val_mse: 0.0557 - 64ms/epoch - 4ms/step\n",
      "Epoch 118/1000\n",
      "17/17 - 0s - loss: 0.0550 - mse: 0.0550 - val_loss: 0.0557 - val_mse: 0.0557 - 58ms/epoch - 3ms/step\n",
      "Epoch 119/1000\n",
      "17/17 - 0s - loss: 0.0568 - mse: 0.0568 - val_loss: 0.0595 - val_mse: 0.0595 - 53ms/epoch - 3ms/step\n",
      "Epoch 120/1000\n",
      "17/17 - 0s - loss: 0.0582 - mse: 0.0582 - val_loss: 0.0565 - val_mse: 0.0565 - 54ms/epoch - 3ms/step\n",
      "Epoch 121/1000\n",
      "17/17 - 0s - loss: 0.0567 - mse: 0.0567 - val_loss: 0.0574 - val_mse: 0.0574 - 51ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24f8e319d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 12\n",
    "output_size = 1\n",
    "hidden_layer_size = 200\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='sigmoid'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='sigmoid'),\n",
    "                            tf.keras.layers.Dense(output_size, activation ='linear'),\n",
    "                            ])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss ='mean_squared_error', metrics =['mse'])\n",
    "\n",
    "batch_size = 128\n",
    "max_epochs = 1000\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 20)\n",
    "model.fit (train_inputs,\n",
    "          train_targets,\n",
    "          batch_size = batch_size,\n",
    "          epochs = max_epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data = (validation_inputs, validation_targets),\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1cd61c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 1ms/step\n",
      "MAPE: 37.36519902534323\n",
      "MSE: 233.68769711139\n",
      "MAE: 12.277608558599532\n",
      "R: 0.6207177841029861\n"
     ]
    }
   ],
   "source": [
    "# Predict the test targets using the trained model\n",
    "test_predictions = model.predict(test_inputs)\n",
    "test_targets_reshaped= test_targets.reshape(-1, 1)\n",
    "\n",
    "# Unscale the predictions and targets\n",
    "test_predictions_unscaled = scaler.inverse_transform(test_predictions)\n",
    "test_targets_unscaled = scaler.inverse_transform(test_targets.reshape(-1, 1))\n",
    "\n",
    "# Calculate the MAPE\n",
    "mape = np.mean(np.abs((test_targets_unscaled - test_predictions_unscaled) / test_targets_unscaled)) * 100\n",
    "\n",
    "# Calculate the RMSE\n",
    "mse = np.mean((test_targets_unscaled - test_predictions_unscaled)**2)\n",
    "\n",
    "# Calculate the MAE\n",
    "mae = np.mean(np.abs(test_targets_unscaled - test_predictions_unscaled))\n",
    "\n",
    "# Calculate the correlation coefficient (R)\n",
    "r = np.corrcoef(test_targets_unscaled.T, test_predictions_unscaled.T)[0,1]\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('MAPE:', mape)\n",
    "print('MSE:', mse)\n",
    "print('MAE:', mae)\n",
    "print('R:', r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space = {\n",
    "    'hidden_layers': hp.choice('hidden_layers', [1, 2, 3, 4]),  # for simplicity, up to 4 hidden layers\n",
    "    'hidden_layer_size': hp.choice('hidden_layer_size', [50, 100, 200, 400]),\n",
    "    'activation': hp.choice('activation', ['relu', 'sigmoid', 'tanh']),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -7, -3)\n",
    "}\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add the hidden layers\n",
    "    for _ in range(params['hidden_layers']):\n",
    "        model.add(tf.keras.layers.Dense(params['hidden_layer_size'], activation=params['activation']))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(output_size, activation='linear'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "    # You can use the early stopping callback if you want\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "\n",
    "    history = model.fit(train_inputs,\n",
    "                        train_targets,\n",
    "                        batch_size=128,  # keeping this fixed for now, can be added to space\n",
    "                        epochs=1000,  # keeping this fixed for now\n",
    "                        callbacks=[early_stopping],\n",
    "                        validation_data=(validation_inputs, validation_targets),\n",
    "                        verbose=0)  # no output\n",
    "\n",
    "    # Return the validation error from the last epoch\n",
    "    val_error = history.history['val_mse'][-1]\n",
    "    return {'loss': val_error, 'status': STATUS_OK}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,  # for example\n",
    "            trials=trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afbff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Continue with the Bayesian Optimization and ANN as provided...\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space = {\n",
    "    'hidden_layers': hp.choice('hidden_layers', [1, 2, 3, 4]),  # for simplicity, up to 4 hidden layers\n",
    "    'hidden_layer_size': hp.choice('hidden_layer_size', [50, 100, 200, 400]),\n",
    "    'activation': hp.choice('activation', ['relu', 'sigmoid', 'tanh']),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -7, -3),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128, 256]),\n",
    "    'learning_rate_schedule': hp.choice('learning_rate_schedule', ['constant', 'step_decay', 'exponential_decay'])\n",
    "}\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add the hidden layers\n",
    "    for _ in range(params['hidden_layers']):\n",
    "        model.add(tf.keras.layers.Dense(params['hidden_layer_size'], activation=params['activation']))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(output_size, activation='linear'))\n",
    "    \n",
    "    # Adjust learning rate based on schedule\n",
    "    if params['learning_rate_schedule'] == 'step_decay':\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            params['learning_rate'],\n",
    "            decay_steps=1000,\n",
    "            decay_rate=0.9,\n",
    "            staircase=True)\n",
    "    elif params['learning_rate_schedule'] == 'exponential_decay':\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            params['learning_rate'],\n",
    "            decay_steps=1000,\n",
    "            decay_rate=0.9,\n",
    "            staircase=False)\n",
    "    else:\n",
    "        lr_schedule = params['learning_rate']\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "    # You can use the early stopping callback if you want\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "    history = model.fit(train_inputs,\n",
    "                        train_targets,\n",
    "                        batch_size=params['batch_size'],  # keeping this fixed for now, can be added to space\n",
    "                        epochs=1000,  # keeping this fixed for now\n",
    "                        callbacks=[early_stopping],\n",
    "                        validation_data=(validation_inputs, validation_targets),\n",
    "                        verbose=0)  # no output\n",
    "\n",
    "    val_error = history.history['val_mse'][-1]\n",
    "\n",
    "    # Return the validation error from the last epoch\n",
    "    val_error = history.history['val_mse'][-1]\n",
    "    return {\n",
    "        'loss': val_error, \n",
    "        'status': STATUS_OK, \n",
    "        'mse': history.history['mse'], \n",
    "        'val_mse': history.history['val_mse']\n",
    "    }\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,  # for example\n",
    "            trials=trials)\n",
    "\n",
    "# Extract mse and val_mse from the best trial\n",
    "best_trial_index = np.argmin(trials.losses())\n",
    "mse_best_trial = trials.trials[best_trial_index]['result']['mse']\n",
    "val_mse_best_trial = trials.trials[best_trial_index]['result']['val_mse']\n",
    "\n",
    "# Convert to dataframe\n",
    "df_mse_best = pd.DataFrame({'mse': mse_best_trial})\n",
    "df_val_mse_best = pd.DataFrame({'val_mse': val_mse_best_trial})\n",
    "\n",
    "# Store data to Excel\n",
    "with pd.ExcelWriter('optimisation_store.xlsx') as writer:\n",
    "    df_mse_best.to_excel(writer, sheet_name='MSE', index=False)\n",
    "    df_val_mse_best.to_excel(writer, sheet_name='Validation MSE', index=False)\n",
    "\n",
    "print(best)\n",
    "\n",
    "# After Bayesian Optimization, retrain the model with best hyperparameters\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "best_params = {\n",
    "    'hidden_layers': [1, 2, 3, 4][best['hidden_layers']],\n",
    "    'hidden_layer_size': [50, 100, 200, 400][best['hidden_layer_size']],\n",
    "    'activation': ['relu', 'sigmoid', 'tanh'][best['activation']],\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'batch_size': [32, 64, 128, 256][best['batch_size']],\n",
    "    'learning_rate_schedule': ['constant', 'step_decay', 'exponential_decay'][best['learning_rate_schedule']]\n",
    "}\n",
    "\n",
    "best_model = tf.keras.Sequential()\n",
    "\n",
    "# Add the hidden layers\n",
    "for _ in range(best_params['hidden_layers']):\n",
    "    best_model.add(tf.keras.layers.Dense(best_params['hidden_layer_size'], activation=best_params['activation']))\n",
    "\n",
    "# Output layer\n",
    "best_model.add(tf.keras.layers.Dense(output_size, activation='linear'))\n",
    "\n",
    "# Adjust learning rate based on best schedule\n",
    "if best_params['learning_rate_schedule'] == 'step_decay':\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        best_params['learning_rate'],\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True)\n",
    "elif best_params['learning_rate_schedule'] == 'exponential_decay':\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        best_params['learning_rate'],\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.9,\n",
    "        staircase=False)\n",
    "else:\n",
    "    lr_schedule = best_params['learning_rate']\n",
    "    \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "best_model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "best_model.fit(train_inputs,\n",
    "               train_targets,\n",
    "               batch_size=best_params['batch_size'],\n",
    "               epochs=1000,\n",
    "               callbacks=[early_stopping],\n",
    "               validation_data=(validation_inputs, validation_targets),\n",
    "               verbose=2)\n",
    "\n",
    "# Make predictions on test_inputs\n",
    "test_predictions = best_model.predict(test_inputs)\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = np.mean(np.abs((test_targets - test_predictions.flatten()) / test_targets)) * 100\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(test_targets, test_predictions)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(test_targets, test_predictions)\n",
    "\n",
    "# Calculate R^2\n",
    "r = np.corrcoef(test_targets_reshaped.T, test_predictions.T)[0,1]\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('MAPE:', mape)\n",
    "print('MSE:', mse)\n",
    "print('MAE:', mae)\n",
    "print('R:', r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bb9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8413e5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9e70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139352f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f368bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d54c961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
