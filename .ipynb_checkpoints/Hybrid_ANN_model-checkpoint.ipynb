{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d42844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Use the sklearn preprocessing library, as it will be easier to standardize the data.\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "raw_excel_data = pd.read_excel('Database.xlsx')\n",
    "\n",
    "# Replace missing values in the columns with their respective mean\n",
    "raw_excel_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values = raw_excel_data.isnull().sum()\n",
    "\n",
    "# Display columns with missing values and their counts\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408c858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs_all = raw_excel_data.iloc[:,0:-1].values\n",
    "\n",
    "# The targets are in the last column. That's how datasets are conventionally organized.\n",
    "unscaled_targets_all = raw_excel_data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdb04c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the StandardScaler object for inputs and targets\n",
    "#scaler_inputs = preprocessing.StandardScaler()\n",
    "#scaler_targets = preprocessing.StandardScaler()\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Continue with scaling and calculations\n",
    "scaled_inputs = scaler.fit_transform(unscaled_inputs_all)\n",
    "scaled_targets = scaler.fit_transform(unscaled_targets_all.reshape(-1, 1))\n",
    "print(scaled_inputs[:5])\n",
    "print(scaled_targets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the data was collected it was actually arranged by date\n",
    "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n",
    "# Since we will be batching, we want the data to be as randomly spread out as possible\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = scaled_targets [shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Count the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Count the samples in each subset, assuming 70-15-15 distribution of training, validation, and test.\n",
    "# Naturally, the numbers are integers.\n",
    "train_samples_count = int(0.7 * samples_count)\n",
    "validation_samples_count = int(0.15 * samples_count)\n",
    "\n",
    "# The 'test' dataset contains all remaining data.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# Create variables that record the inputs and targets for training\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 12\n",
    "output_size = 1\n",
    "hidden_layer_size = 200\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='sigmoid'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='sigmoid'),\n",
    "                            tf.keras.layers.Dense(output_size, activation ='linear'),\n",
    "                            ])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss ='mean_squared_error', metrics =['mse'])\n",
    "\n",
    "batch_size = 128\n",
    "max_epochs = 1000\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 20)\n",
    "model.fit (train_inputs,\n",
    "          train_targets,\n",
    "          batch_size = batch_size,\n",
    "          epochs = max_epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data = (validation_inputs, validation_targets),\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test targets using the trained model\n",
    "test_predictions = model.predict(test_inputs)\n",
    "test_targets_reshaped= test_targets.reshape(-1, 1)\n",
    "\n",
    "# Unscale the predictions and targets\n",
    "test_predictions_unscaled = scaler.inverse_transform(test_predictions)\n",
    "test_targets_unscaled = scaler.inverse_transform(test_targets.reshape(-1, 1))\n",
    "\n",
    "# Calculate the MAPE\n",
    "mape = np.mean(np.abs((test_targets_unscaled - test_predictions_unscaled) / test_targets_unscaled)) * 100\n",
    "\n",
    "# Calculate the RMSE\n",
    "mse = np.mean((test_targets_unscaled - test_predictions_unscaled)**2)\n",
    "\n",
    "# Calculate the MAE\n",
    "mae = np.mean(np.abs(test_targets_unscaled - test_predictions_unscaled))\n",
    "\n",
    "# Calculate the correlation coefficient (R)\n",
    "r = np.corrcoef(test_targets_unscaled.T, test_predictions_unscaled.T)[0,1]\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('MAPE:', mape)\n",
    "print('MSE:', mse)\n",
    "print('MAE:', mae)\n",
    "print('R:', r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afbff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Continue with the Bayesian Optimization and ANN as provided...\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space = {\n",
    "    'hidden_layers': hp.choice('hidden_layers', [1, 2, 3, 4]),  # for simplicity, up to 4 hidden layers\n",
    "    'hidden_layer_size': hp.choice('hidden_layer_size', [50, 100, 200, 400]),\n",
    "    'activation': hp.choice('activation', ['relu', 'sigmoid', 'tanh']),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -7, -3),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128, 256]),\n",
    "    'learning_rate_schedule': hp.choice('learning_rate_schedule', ['constant', 'step_decay', 'exponential_decay'])\n",
    "}\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add the hidden layers\n",
    "    for _ in range(params['hidden_layers']):\n",
    "        model.add(tf.keras.layers.Dense(params['hidden_layer_size'], activation=params['activation']))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(output_size, activation='linear'))\n",
    "    \n",
    "    # Adjust learning rate based on schedule\n",
    "    if params['learning_rate_schedule'] == 'step_decay':\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            params['learning_rate'],\n",
    "            decay_steps=1000,\n",
    "            decay_rate=0.9,\n",
    "            staircase=True)\n",
    "    elif params['learning_rate_schedule'] == 'exponential_decay':\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            params['learning_rate'],\n",
    "            decay_steps=1000,\n",
    "            decay_rate=0.9,\n",
    "            staircase=False)\n",
    "    else:\n",
    "        lr_schedule = params['learning_rate']\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "    # You can use the early stopping callback if you want\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "    history = model.fit(train_inputs,\n",
    "                        train_targets,\n",
    "                        batch_size=params['batch_size'],  # keeping this fixed for now, can be added to space\n",
    "                        epochs=1000,  # keeping this fixed for now\n",
    "                        callbacks=[early_stopping],\n",
    "                        validation_data=(validation_inputs, validation_targets),\n",
    "                        verbose=0)  # no output\n",
    "\n",
    "    val_error = history.history['val_mse'][-1]\n",
    "\n",
    "    # Return the validation error from the last epoch\n",
    "    val_error = history.history['val_mse'][-1]\n",
    "    return {\n",
    "        'loss': val_error, \n",
    "        'status': STATUS_OK, \n",
    "        'mse': history.history['mse'], \n",
    "        'val_mse': history.history['val_mse']\n",
    "    }\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,  # for example\n",
    "            trials=trials)\n",
    "\n",
    "# Extract mse and val_mse from the best trial\n",
    "best_trial_index = np.argmin(trials.losses())\n",
    "mse_best_trial = trials.trials[best_trial_index]['result']['mse']\n",
    "val_mse_best_trial = trials.trials[best_trial_index]['result']['val_mse']\n",
    "\n",
    "# Convert to dataframe\n",
    "df_mse_best = pd.DataFrame({'mse': mse_best_trial})\n",
    "df_val_mse_best = pd.DataFrame({'val_mse': val_mse_best_trial})\n",
    "\n",
    "# Store data to Excel\n",
    "with pd.ExcelWriter('optimisation_store.xlsx') as writer:\n",
    "    df_mse_best.to_excel(writer, sheet_name='MSE', index=False)\n",
    "    df_val_mse_best.to_excel(writer, sheet_name='Validation MSE', index=False)\n",
    "\n",
    "print(best)\n",
    "\n",
    "# After Bayesian Optimization, retrain the model with best hyperparameters\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "best_params = {\n",
    "    'hidden_layers': [1, 2, 3, 4][best['hidden_layers']],\n",
    "    'hidden_layer_size': [50, 100, 200, 400][best['hidden_layer_size']],\n",
    "    'activation': ['relu', 'sigmoid', 'tanh'][best['activation']],\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'batch_size': [32, 64, 128, 256][best['batch_size']],\n",
    "    'learning_rate_schedule': ['constant', 'step_decay', 'exponential_decay'][best['learning_rate_schedule']]\n",
    "}\n",
    "\n",
    "best_model = tf.keras.Sequential()\n",
    "\n",
    "# Add the hidden layers\n",
    "for _ in range(best_params['hidden_layers']):\n",
    "    best_model.add(tf.keras.layers.Dense(best_params['hidden_layer_size'], activation=best_params['activation']))\n",
    "\n",
    "# Output layer\n",
    "best_model.add(tf.keras.layers.Dense(output_size, activation='linear'))\n",
    "\n",
    "# Adjust learning rate based on best schedule\n",
    "if best_params['learning_rate_schedule'] == 'step_decay':\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        best_params['learning_rate'],\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True)\n",
    "elif best_params['learning_rate_schedule'] == 'exponential_decay':\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        best_params['learning_rate'],\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.9,\n",
    "        staircase=False)\n",
    "else:\n",
    "    lr_schedule = best_params['learning_rate']\n",
    "    \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "best_model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "best_model.fit(train_inputs,\n",
    "               train_targets,\n",
    "               batch_size=best_params['batch_size'],\n",
    "               epochs=1000,\n",
    "               callbacks=[early_stopping],\n",
    "               validation_data=(validation_inputs, validation_targets),\n",
    "               verbose=2)\n",
    "\n",
    "# Make predictions on test_inputs\n",
    "test_predictions = best_model.predict(test_inputs)\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = np.mean(np.abs((test_targets - test_predictions.flatten()) / test_targets)) * 100\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(test_targets, test_predictions)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(test_targets, test_predictions)\n",
    "\n",
    "# Calculate R^2\n",
    "r = np.corrcoef(test_targets_reshaped.T, test_predictions.T)[0,1]\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('MAPE:', mape)\n",
    "print('MSE:', mse)\n",
    "print('MAE:', mae)\n",
    "print('R:', r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
